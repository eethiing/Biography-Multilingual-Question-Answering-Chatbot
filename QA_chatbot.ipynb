{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20382,"status":"ok","timestamp":1686935308638,"user":{"displayName":"VINCENT TAN","userId":"03279800603788297784"},"user_tz":-480},"id":"2vbQsc5hQBeV","outputId":"96e66fb3-6563-4b25-f8b5-e97bb94bc763"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"IPIbiNsWQIOV"},"source":["Installing Haystack and malaya"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjm9MpHvt6sr"},"outputs":[],"source":["!pip install malaya"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6648,"status":"ok","timestamp":1686935335398,"user":{"displayName":"VINCENT TAN","userId":"03279800603788297784"},"user_tz":-480},"id":"u8mwNYCRmQPv","outputId":"194d1d3d-c444-478e-b0d9-328d45427dad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=4f61d98fa2b2cea6a35a29cd150faa9320da917c840e9289675bcfaabc234c77\n","  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n","Successfully built sacremoses\n","Installing collected packages: sacremoses\n","Successfully installed sacremoses-0.0.53\n"]}],"source":["!pip install sacremoses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwzAD23uQFKb"},"outputs":[],"source":["%%bash\n","\n","pip install --upgrade pip\n","pip install farm-haystack[colab]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNM16TNtJVmp"},"outputs":[],"source":["!pip install transformers\n","!pip install torch\n","!pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jGsfaL0b6QA"},"outputs":[],"source":["!pip install langdetect"]},{"cell_type":"markdown","metadata":{"id":"AuyGYq7gpLzk"},"source":["Read Document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BjYRCNCrqS5z"},"outputs":[],"source":["import json\n","\n","documents = []\n","path = '/content/drive/Shareddrives/NLP/Project/documents_v2.json'\n","path = '/content/drive/MyDrive/Trimester 2 2022 2023/TNL3221 Natural Language Processing/Project/documents_v2.json'\n","# Read all documents from the local JSON file\n","with open(path, 'r', encoding='utf-8') as f:\n","    docs = json.load(f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIVDr5iRVe9h"},"outputs":[],"source":["documents = []\n","for i in range(len(docs)) :\n","  documents.append(docs[i]['content'])"]},{"cell_type":"markdown","metadata":{"id":"eRMiLEcwc2Bi"},"source":["Translate Ques to English"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEMoIWxYZU-_"},"outputs":[],"source":["# translate ques to eng\n","import malaya\n","from langdetect import detect\n","from transformers import MarianMTModel, MarianTokenizer\n","\n","model_de = 'Helsinki-NLP/opus-mt-de-en'\n","model_fr = 'Helsinki-NLP/opus-mt-fr-en'\n","model_it = 'Helsinki-NLP/opus-mt-it-en'\n","\n","tokenizer_de = MarianTokenizer.from_pretrained(model_de)\n","model_de = MarianMTModel.from_pretrained(model_de)\n","tokenizer_fr = MarianTokenizer.from_pretrained(model_fr)\n","model_fr = MarianMTModel.from_pretrained(model_fr)\n","tokenizer_it = MarianTokenizer.from_pretrained(model_it)\n","model_it = MarianMTModel.from_pretrained(model_it)\n","\n","#transformer_ms = malaya.translation.ms_en.transformer()\n","transformer_small_ms = malaya.translation.ms_en.transformer(model = 'small')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFWJrnp9gfPu"},"outputs":[],"source":["# translate\n","def translate_ms_en(text):\n","    return transformer_small_ms.greedy_decoder([text])\n","\n","def translate_ques_to_eng(text):\n","    lang = detect(text)\n","\n","    if lang == 'de':\n","        # Use German to English model\n","        inputs = tokenizer_de(text, return_tensors=\"pt\", padding=True)\n","        translated = model_de.generate(**inputs)\n","        translated_text = [tokenizer_de.decode(t, skip_special_tokens=True) for t in translated]\n","        print()\n","    elif lang == 'fr':\n","        # Use French to English model\n","        inputs = tokenizer_fr(text, return_tensors=\"pt\", padding=True)\n","        translated = model_fr.generate(**inputs)\n","        translated_text = [tokenizer_fr.decode(t, skip_special_tokens=True) for t in translated]\n","    elif lang == 'it':\n","        # Use Italian to English model\n","        inputs = tokenizer_it(text, return_tensors=\"pt\", padding=True)\n","        translated = model_it.generate(**inputs)\n","        translated_text = [tokenizer_it.decode(t, skip_special_tokens=True) for t in translated]\n","    elif lang == 'ms' or lang == 'id' or lang == 'id':\n","        # Use Malay to English model\n","        translated_text = translate_ms_en(text)\n","        #translated_text = tokenizer_ms.tokenize(translated_text)\n","    else:\n","        # If the language is neither of the above languages, return the original text\n","        return text\n","    return translated_text[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Be7IEgOkLLoF"},"outputs":[],"source":["import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Tokenize sentences\n","nltk.download('punkt') # first-time use only\n","nltk.download('stopwords') # first-time use only\n","\n","# Preprocessing\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_ques(doc):\n","    # check if the document is in english\n","    if detect(doc) != 'en':\n","      doc = translate_ques_to_eng(doc)\n","    word_tokens = word_tokenize(doc)\n","    # lowercase\n","    word_tokens = [word.lower() for word in word_tokens]\n","    # remove stopwords\n","    filtered_text = [word for word in word_tokens if word.casefold() not in stop_words]\n","    # Remove punctuation\n","    punctuations = ['(', ')', ';',':','\"']\n","    filtered_text = [word for word in filtered_text if filtered_text not in punctuations]\n","    return ' '.join(filtered_text)\n","\n","# Filter out non-string documents\n","documents = [doc for doc in documents if isinstance(doc, str)]\n","\n","# Now preprocess the documents\n","# documents = [preprocess_text(doc) for doc in documents]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7O7udZawtnzq"},"outputs":[],"source":["# third model # best model\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Initialize the BERT model (this will download the model the first time)\n","model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n","\n","# Generate embeddings for each document\n","embeddings = model.encode(documents)\n","\n","def ask_question(query):\n","    query = preprocess_ques(query)\n","\n","    # Generate the BERT embedding for the query\n","    query_embedding = model.encode([query])\n","\n","    # Compute the cosine similarity between the query embedding and all document embeddings\n","    results = cosine_similarity(embeddings, query_embedding).flatten()\n","\n","    # Get the index of the most similar document\n","    most_similar_doc_index = results.argsort()[-1]\n","\n","    return documents[most_similar_doc_index]\n","\n","# Testing\n","ques = \"quand est né barack obama?\"\n","answer = ask_question(ques)\n","print(answer)"]},{"cell_type":"markdown","metadata":{"id":"brLyNenGoApS"},"source":["Translate Answer to Malay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CDiYBj7dk04"},"outputs":[],"source":["# translate ques to eng\n","import malaya\n","from langdetect import detect\n","from transformers import MarianMTModel, MarianTokenizer\n","\n","model_en_de = 'Helsinki-NLP/opus-mt-en-de'\n","model_en_fr = 'Helsinki-NLP/opus-mt-en-fr'\n","model_en_it = 'Helsinki-NLP/opus-mt-en-it'\n","\n","tokenizer_en_de = MarianTokenizer.from_pretrained(model_en_de)\n","model_en_de = MarianMTModel.from_pretrained(model_en_de)\n","tokenizer_en_fr = MarianTokenizer.from_pretrained(model_en_fr)\n","model_en_fr = MarianMTModel.from_pretrained(model_en_fr)\n","tokenizer_en_it = MarianTokenizer.from_pretrained(model_en_it)\n","model_en_it = MarianMTModel.from_pretrained(model_en_it)\n","\n","#transformer_ms = malaya.translation.ms_en.transformer()\n","transformer_small = malaya.translation.en_ms.transformer(model = 'small')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULp4pZEVdnt-"},"outputs":[],"source":["# translate\n","def translate_en_ms(text):\n","    return transformer_small.greedy_decoder([text])\n","\n","def translate_ans_to_eng(ques,ans):\n","    lang = detect(ques)\n","\n","    if lang == 'de':\n","        # Use English to German model\n","        inputs = tokenizer_en_de(ans, return_tensors=\"pt\", padding=True)\n","        translated = model_en_de.generate(**inputs)\n","        translated_text = [tokenizer_en_de.decode(t, skip_special_tokens=True) for t in translated]\n","    elif lang == 'fr':\n","        # Use English to French model\n","        inputs = tokenizer_en_fr(ans, return_tensors=\"pt\", padding=True)\n","        translated = model_en_fr.generate(**inputs)\n","        translated_text = [tokenizer_en_fr.decode(t, skip_special_tokens=True) for t in translated]\n","    elif lang == 'it':\n","        # Use English to Italian model\n","        inputs = tokenizer_en_it(ans, return_tensors=\"pt\", padding=True)\n","        translated = model_en_it.generate(**inputs)\n","        translated_text = [tokenizer_en_it.decode(t, skip_special_tokens=True) for t in translated]\n","    elif lang == 'ms' or lang == 'id' or lang == 'id':\n","        # Use English to Malay model\n","        translated_text = translate_en_ms(ans)\n","        #translated_text = tokenizer_ms.tokenize(translated_text)\n","    else:\n","        # If the language is neither of the above languages, return the original text\n","        return ans\n","    return translated_text[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAWZBE2rezuP"},"outputs":[],"source":["translated_answer = translate_ans_to_eng(ques, answer)\n","print(\"EN Answer:\\n\", answer)\n","print(\"Translated Answer:\\n\", translated_answer)"]},{"cell_type":"markdown","metadata":{"id":"XeVn0UJUF6rB"},"source":["Deploy the question answering system on a Fast API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDXGbER2K5Tw"},"outputs":[],"source":["!pip install fastapi uvicorn pyngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LOWkEgAWGpc-"},"outputs":[],"source":["from fastapi import FastAPI\n","from fastapi.middleware.cors import CORSMiddleware\n","from pyngrok import ngrok\n","import uvicorn\n","import nest_asyncio\n","\n","app = FastAPI()\n","\n","origins = [\"*\"]\n","\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=origins,\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","# Endpoint to get an answer to a question\n","@app.get('/get_answer/{question}')\n","async def get_answer(question: str):\n","    # Get the answer to the question\n","    answer = ask_question(question)\n","\n","    # Translate the answer to the language of the question\n","    final_answer = translate_ans_to_eng(question,answer)\n","    return final_answer\n","\n","# Endpoint to check the connection status\n","@app.get('/check')\n","async def check():\n","    return 'connected'\n","\n","# Connect to ngrok for tunneling\n","ngrok_tunnel = ngrok.connect(8000)\n","print('Public URL:', ngrok_tunnel.public_url)\n","nest_asyncio.apply()\n","uvicorn.run(app, port=8000)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1tQQhmQ-BOh-Zq-iK2uKKR5jefNI8FDou","timestamp":1686991416958},{"file_id":"13kT_Lj7sv44KTf2ilOWfC9CtXdy-hm5k","timestamp":1686581144445}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}